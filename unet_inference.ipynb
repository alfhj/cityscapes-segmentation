{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet_model import *\n",
    "from glob import glob\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from datetime import datetime\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ignite.metrics import ConfusionMatrix, mIoU\n",
    "from ignite.engine import Engine\n",
    "from torchmetrics import JaccardIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "train_path = glob('/cluster/projects/vc/data/ad/open/Cityscapes/leftImg8bit_trainvaltest/leftImg8bit/train/*/*leftImg8bit.png')\n",
    "val_path = glob('/cluster/projects/vc/data/ad/open/Cityscapes/leftImg8bit_trainvaltest/leftImg8bit/val/*/*leftImg8bit.png')\n",
    "len(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedLabel = namedtuple(\"GroupedLabel\", [\n",
    "    \"id\", # group id\n",
    "    \"name\", # group name\n",
    "    \"ids\", # list of ids\n",
    "    \"color\", # color of the group\n",
    "])\n",
    "# grouped_labels = [\n",
    "#     GroupedLabel(0, \"motor vehicles\" , [26, 27, 28, 28, 29, 30, 31, 32],           (  0,   0, 142)),\n",
    "#     GroupedLabel(1, \"pedestrians\"    , [24, 25, 33],                               (220,  20,  60)),\n",
    "#     GroupedLabel(2, \"road\"           , [6, 7, 8, 9, 10],                           (128,  64, 128)),\n",
    "#     GroupedLabel(3, \"traffic objects\", [17, 18, 19, 20],                           (250, 170,  30)),\n",
    "#     GroupedLabel(4, \"background\"     , [4, 5, 11, 12, 13, 14, 15, 16, 21, 22, 23], ( 70,  70,  70)),\n",
    "#     GroupedLabel(5, \"void\"           , [0, 2, 3],                                  (  0,   0,   0)),\n",
    "#     GroupedLabel(6, \"ego vehicle\"    , [1],                                        (  0,   0,   0)),\n",
    "# ]\n",
    "grouped_labels = [\n",
    "    GroupedLabel(id=0, name='unlabeled', ids=[0, 1, 2, 3, 4], color=(0, 0, 0)),\n",
    "    GroupedLabel(id=1, name='dynamic', ids=[5], color=(111, 74, 0)),\n",
    "    GroupedLabel(id=2, name='ground', ids=[6], color=(81, 0, 81)),\n",
    "    GroupedLabel(id=3, name='road', ids=[7], color=(128, 64, 128)),\n",
    "    GroupedLabel(id=4, name='sidewalk', ids=[8], color=(244, 35, 232)),\n",
    "    GroupedLabel(id=5, name='parking', ids=[9], color=(250, 170, 160)),\n",
    "    GroupedLabel(id=6, name='rail track', ids=[10], color=(230, 150, 140)),\n",
    "    GroupedLabel(id=7, name='building', ids=[11], color=(70, 70, 70)),\n",
    "    GroupedLabel(id=8, name='wall', ids=[12], color=(102, 102, 156)),\n",
    "    GroupedLabel(id=9, name='fence', ids=[13], color=(190, 153, 153)),\n",
    "    GroupedLabel(id=10, name='guard rail', ids=[14], color=(180, 165, 180)),\n",
    "    GroupedLabel(id=11, name='bridge', ids=[15], color=(150, 100, 100)),\n",
    "    GroupedLabel(id=12, name='tunnel', ids=[16], color=(150, 120, 90)),\n",
    "    GroupedLabel(id=13, name='pole', ids=[17, 18], color=(153, 153, 153)),\n",
    "    GroupedLabel(id=14, name='traffic light', ids=[19], color=(250, 170, 30)),\n",
    "    GroupedLabel(id=15, name='traffic sign', ids=[20], color=(220, 220, 0)),\n",
    "    GroupedLabel(id=16, name='vegetation', ids=[21], color=(107, 142, 35)),\n",
    "    GroupedLabel(id=17, name='terrain', ids=[22], color=(152, 251, 152)),\n",
    "    GroupedLabel(id=18, name='sky', ids=[23], color=(70, 130, 180)),\n",
    "    GroupedLabel(id=19, name='person', ids=[24], color=(220, 20, 60)),\n",
    "    GroupedLabel(id=20, name='rider', ids=[25], color=(255, 0, 0)),\n",
    "    GroupedLabel(id=21, name='car', ids=[26, -1], color=(0, 0, 142)),\n",
    "    GroupedLabel(id=22, name='truck', ids=[27], color=(0, 0, 70)),\n",
    "    GroupedLabel(id=23, name='bus', ids=[28], color=(0, 60, 100)),\n",
    "    GroupedLabel(id=24, name='caravan', ids=[29], color=(0, 0, 90)),\n",
    "    GroupedLabel(id=25, name='trailer', ids=[30], color=(0, 0, 110)),\n",
    "    GroupedLabel(id=26, name='train', ids=[31], color=(0, 80, 100)),\n",
    "    GroupedLabel(id=27, name='motorcycle', ids=[32], color=(0, 0, 230)),\n",
    "    GroupedLabel(id=28, name='bicycle', ids=[33], color=(119, 11, 32))\n",
    " ]\n",
    "\n",
    "def classes_to_rgb(output: torch.Tensor) -> torch.Tensor:\n",
    "    # Input: (num_classes, H, W)\n",
    "    # Output: (3, H, W)\n",
    "    rgb = torch.zeros((3, *output.size()[-2:]))\n",
    "    output_max = torch.argmax(output.squeeze(), dim=0)\n",
    "    for label in grouped_labels:\n",
    "        for c in range(3):\n",
    "            rgb[c][output_max == label.id] = label.color[c] / 255\n",
    "    return rgb\n",
    "\n",
    "def gt_to_classes(gt: np.ndarray) -> torch.Tensor:\n",
    "    # Input: (H, W), values: 0-num_classes\n",
    "    # Output: (num_classes, H, W)\n",
    "    output = torch.zeros((len(grouped_labels), *gt.shape))\n",
    "    for group in grouped_labels:\n",
    "        output[group.id] = torch.Tensor(np.isin(gt, group.ids))\n",
    "    return output\n",
    "\n",
    "def gt_to_masks(gt: np.ndarray) -> torch.Tensor:\n",
    "    # Input: (H, W), values: 0-num_classes\n",
    "    # Output: (num_classes, H, W)\n",
    "    pass\n",
    "\n",
    "def infer(unet_model, img_path, out_path):\n",
    "    label_path = img_path.replace(\"leftImg8bit\", \"gtFine\").replace(\".png\", \"_labelIds.png\")\n",
    "    img = np.array(Image.open(img_path))\n",
    "    label = np.array(Image.open(label_path))\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((h//2, w//2), antialias=True),\n",
    "        #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    ])\n",
    "    transform1 = transforms.Compose([\n",
    "        transforms.Resize((h//2, w//2), antialias=True),\n",
    "    ])\n",
    "\n",
    "    img_in = transform(img).to(device)[None]\n",
    "    label_in = transform1(gt_to_classes(label)).to(device)[None]\n",
    "    output = unet_model(img_in)\n",
    "    output = classes_to_rgb(output)\n",
    "    label_in = classes_to_rgb(label_in)\n",
    "    with torch.no_grad():\n",
    "        output = output.cpu().detach().numpy().transpose(1, 2, 0).clip(0, 1)\n",
    "    img_in = img_in.cpu().detach()[0].numpy().transpose(1, 2, 0)\n",
    "    label_in = label_in.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    plt.imsave(out_path, output)\n",
    "    return img_in, output, label_in\n",
    "    # fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n",
    "    # ax[0].imshow(img_in)\n",
    "    # ax[1].imshow(output)\n",
    "    # ax[2].imshow(label)\n",
    "    # plt.show()\n",
    "\n",
    "def infer_metric(unet_model, img_path):\n",
    "    label_path = img_path.replace(\"leftImg8bit\", \"gtFine\").replace(\".png\", \"_labelIds.png\")\n",
    "    img = np.array(Image.open(img_path))\n",
    "    label = np.array(Image.open(label_path))\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((h//2, w//2), antialias=True),\n",
    "        #transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    ])\n",
    "    transform1 = transforms.Compose([\n",
    "        #transforms.ToTensor(),\n",
    "        transforms.Resize((h//2, w//2), antialias=True, interpolation=InterpolationMode.NEAREST),\n",
    "    ])\n",
    "\n",
    "    img_in = transform(img).to(device)[None]\n",
    "    label_in = transform1(gt_to_classes(label)).long().to(device)[None]\n",
    "    #label_in = transform1(label).long().to(device)#[None]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = unet_model(img_in)\n",
    "        argmax = torch.argmax(output.squeeze(), dim=0)\n",
    "        ious = []\n",
    "        intersections = 0\n",
    "        unions = 0\n",
    "        for i in range(len(grouped_labels)):\n",
    "            output[0][i] = argmax == i\n",
    "            pred = argmax == i\n",
    "            y = label_in[0][i]\n",
    "            intersection = torch.bitwise_and(pred, y).sum().item()\n",
    "            union = torch.bitwise_or(pred, y).sum().item()\n",
    "            intersections += intersection\n",
    "            unions += union\n",
    "            if union == 0:\n",
    "                union = 1\n",
    "            if intersection:\n",
    "                iou = intersection/union\n",
    "                #print(iou)\n",
    "                ious.append(iou)\n",
    "        iou = sum(ious) / len(ious)\n",
    "        accuracy = intersections / (label_in.size()[-1] * label_in.size()[-2])\n",
    "        # print(label_in.size()[-1] * label_in.size()[-2])\n",
    "        # print(accuracy)\n",
    "        # print()\n",
    "        print(intersections / unions)\n",
    "        # print()\n",
    "        print(iou)\n",
    "    \n",
    "    # default_evaluator = Engine(lambda e, b: b)\n",
    "    # cm = ConfusionMatrix(num_classes=len(grouped_labels))\n",
    "    # metric = mIoU(cm, ignore_index=0)\n",
    "    # metric.attach(default_evaluator, 'miou')\n",
    "    # state = default_evaluator.run([[output, label_in]])\n",
    "    # miou = state.metrics['miou']\n",
    "    \n",
    "    #jaccard = JaccardIndex(task=\"multilabel\", num_labels=len(grouped_labels)).to(device)\n",
    "    jaccard = JaccardIndex(task=\"multiclass\", num_classes=len(grouped_labels)).to(device)\n",
    "    iou = jaccard(output, label_in)\n",
    "    miou = iou.item()\n",
    "\n",
    "    return miou, accuracy\n",
    "\n",
    "def infer_output(unet_model, img_path, out_path):\n",
    "    img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Resize((1024, 512)), antialias=True),\n",
    "    ])\n",
    "\n",
    "    img_in = transform(img).to(device)[None]\n",
    "    with torch.no_grad():\n",
    "        output = unet_model(img_in)\n",
    "    output = classes_to_rgb(output)\n",
    "    output = output.cpu().detach().numpy().transpose(1, 2, 0).clip(0, 1)\n",
    "    img_in = img_in.cpu().detach()[0].numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    plt.imsave(out_path, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7782571460066309\n",
      "0.5786729949031086\n",
      "0.8846948146820068 0.0\n"
     ]
    }
   ],
   "source": [
    "models = Path(\"weights\").glob(\"unetsegment_231127_031540_checkpoint_*.pt\")\n",
    "models = sorted(models, key=lambda x: int(x.name.split(\"_\")[-1].split(\".\")[0]), reverse=True)\n",
    "\n",
    "#img = random.choice(val_path)\n",
    "for model in models:\n",
    "    #print(f\"Saving {model} output\")\n",
    "    unet_model = UNet(3, len(grouped_labels)).float().to(device)\n",
    "    checkpoint = torch.load(model)\n",
    "    unet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #_, _, label = infer(unet_model, img, f\"output/{model.stem}.png\")\n",
    "    mious = []\n",
    "    accs = []\n",
    "    random.shuffle(val_path)\n",
    "    for img in val_path:\n",
    "        miou, accuracy = infer_metric(unet_model, img)\n",
    "        mious.append(miou)\n",
    "        accs.append(accuracy)\n",
    "        print(np.array(mious).mean(), np.array(mious).std())\n",
    "        #print(np.array(accs).mean(), np.array(accs).std())\n",
    "        break\n",
    "    break\n",
    "#plt.imsave(f\"output/{model.stem}_gt.png\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"weights/unetsegment_231127_031540_checkpoint_245.pt\"\n",
    "img = Path(\"misc\").joinpath(\"Trondheim5.png\")\n",
    "unet_model = UNet(3, len(grouped_labels)).float().to(device)\n",
    "checkpoint = torch.load(model)\n",
    "unet_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "infer_output(unet_model, img, f\"misc/{img.stem}_out.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1-static https://johnvansickle.com/ffmpeg/  Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 8 (Debian 8.3.0-6)\n",
      "  configuration: --enable-gpl --enable-version3 --enable-static --disable-debug --disable-ffplay --disable-indev=sndio --disable-outdev=sndio --cc=gcc --enable-fontconfig --enable-frei0r --enable-gnutls --enable-gmp --enable-libgme --enable-gray --enable-libaom --enable-libfribidi --enable-libass --enable-libvmaf --enable-libfreetype --enable-libmp3lame --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-librubberband --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libvorbis --enable-libopus --enable-libtheora --enable-libvidstab --enable-libvo-amrwbenc --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libdav1d --enable-libxvid --enable-libzvbi --enable-libzimg\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, image2, from 'output/unetsegment_231127_031540_checkpoint_%d.png':\n",
      "  Duration: 00:00:09.80, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: png, rgba(pc, gbr/unknown/unknown), 1024x512 [SAR 3937:3937 DAR 2:1], 25 fps, 25 tbr, 25 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mprofile High, level 4.0, 4:2:0, 8-bit\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0m264 - core 164 r3161 a354f11 - H.264/MPEG-4 AVC codec - Copyleft 2003-2023 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=0.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=0 threads=32 lookahead_threads=5 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=13.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'output/video_231127_174535.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 2048x1024 [SAR 1:1 DAR 2:1], q=2-31, 30 fps, 15360 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "\u001b[1;35m[out#0/mp4 @ 0x840b7c0] \u001b[0mvideo:4935kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.083113%\n",
      "frame=  273 fps= 71 q=-1.0 Lsize=    4939kB time=00:00:09.00 bitrate=4495.6kbits/s dup=141 drop=114 speed=2.35x    \n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mframe I:3     Avg QP: 5.96  size: 88782\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mframe P:70    Avg QP:13.39  size: 39231\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mframe B:200   Avg QP:16.53  size: 10201\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mconsecutive B-frames:  1.8%  0.7%  2.2% 95.2%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mmb I  I16..4: 65.6% 14.4% 19.9%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mmb P  I16..4:  5.7%  0.9%  4.1%  P16..4:  8.3%  4.5%  2.4%  0.0%  0.0%    skip:73.9%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mmb B  I16..4:  0.3%  0.1%  0.9%  B16..8:  7.4%  1.6%  0.5%  direct: 0.5%  skip:88.6%  L0:48.1% L1:50.7% BI: 1.2%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0m8x8 transform intra:9.1% inter:4.2%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mcoded y,uvDC,uvAC intra: 23.7% 43.0% 41.5% inter: 1.5% 3.6% 3.5%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mi16 v,h,dc,p: 85% 13%  1%  0%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 30%  7% 61%  0%  0%  0%  0%  0%  0%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 34% 23% 29%  3%  2%  2%  2%  3%  3%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mi8c dc,h,v,p: 64% 18% 15%  3%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mref P L0: 58.7%  1.3% 23.0% 17.0%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mref B L0: 72.3% 17.9%  9.8%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mref B L1: 94.9%  5.1%\n",
      "\u001b[1;36m[libx264 @ 0x849b200] \u001b[0mkb/s:4441.92\n"
     ]
    }
   ],
   "source": [
    "! ffmpeg -y -f image2 -i \"output/unetsegment_231127_031540_checkpoint_%d.png\" -c:v libx264 -vf \"setpts=(3.0-0.01*N)*PTS,hqx=2\" -r 30 -pix_fmt yuv420p -crf 13 -x264-params psy-rd=0 output/video_{datetime.now().strftime(r\"%y%m%d_%H%M%S\")}.mp4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
